{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO3LlHGurJQv15Jun/y2qFf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahbubhimel/Transformer-NLP/blob/main/Multihead_Attention_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10OpK4CvLJ0C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 4\n",
        "batch_size = 1\n",
        "input_dim = 512\n",
        "d_model = 512\n",
        "x = tf.random.normal( (batch_size, sequence_length, input_dim) )"
      ],
      "metadata": {
        "id": "_v7TnYjfLgwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "id": "_F8I3e_kLoDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qkv_layer = tf.keras.layers.Dense(units=3 * d_model, activation=None, use_bias=True, input_shape=(input_dim,))"
      ],
      "metadata": {
        "id": "nPfeSk0nNQIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qkv = qkv_layer(x)"
      ],
      "metadata": {
        "id": "YjXG0datNU6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qkv"
      ],
      "metadata": {
        "id": "Q7Sj0r7-OfYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_heads = 8\n",
        "head_dim = d_model // num_heads\n",
        "qkv = tf.reshape(qkv, (batch_size, sequence_length, num_heads, 3 * head_dim))"
      ],
      "metadata": {
        "id": "NhDoXg42Qgqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qkv"
      ],
      "metadata": {
        "id": "CHaeDtK1YC6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qkv = tf.transpose(qkv, perm=[0, 2, 1, 3])"
      ],
      "metadata": {
        "id": "34smmoo4ZixS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qkv"
      ],
      "metadata": {
        "id": "icfieLzsXMBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q, k, v = tf.split(qkv, num_or_size_splits=3, axis=3)"
      ],
      "metadata": {
        "id": "K-ia45XQX18z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q.shape, k.shape, v.shape"
      ],
      "metadata": {
        "id": "63JG_ziaZENH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_transposed = tf.transpose(k, perm = [0, 1, 3, 2])"
      ],
      "metadata": {
        "id": "tjcvtftMbsuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = q.shape[-1]\n",
        "\n",
        "scaled = tf.matmul(q, k_transposed) / math.sqrt(d_k)\n",
        "scaled.shape"
      ],
      "metadata": {
        "id": "1jSk-ruCbw5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled"
      ],
      "metadata": {
        "id": "cq7OH4Gimr7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mask with values set to negative infinity\n",
        "mask = np.full(scaled.shape, -np.inf)\n",
        "\n",
        "# Create an upper triangular mask by zeroing out values below the diagonal\n",
        "mask = np.triu(mask, k=1)  # Keeps values above the diagonal"
      ],
      "metadata": {
        "id": "sqVUXUnCb4Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask"
      ],
      "metadata": {
        "id": "MvSR7vf0l6ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled = scaled + mask"
      ],
      "metadata": {
        "id": "qezooRtWnZvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled[0][0]"
      ],
      "metadata": {
        "id": "jNWj4T84ngi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.exp(-0.5100664)/(np.exp(0.06729835)+np.exp(-0.5100664))"
      ],
      "metadata": {
        "id": "3jt2RgRytIn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention = tf.nn.softmax(scaled, axis=-1)"
      ],
      "metadata": {
        "id": "cHfhOSk9nkTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention.shape"
      ],
      "metadata": {
        "id": "czV7yq1Op9Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention[0][0]"
      ],
      "metadata": {
        "id": "I1hhcnOAp-k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values = tf.matmul(attention, v)\n",
        "values.shape"
      ],
      "metadata": {
        "id": "CCExiP4UvRPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values"
      ],
      "metadata": {
        "id": "NL2C4EG3vtLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attention Function**"
      ],
      "metadata": {
        "id": "BnlcSYMKxagp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "f7Tgcgr5yC1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "  d_k = q.shape[-1]\n",
        "  k_transposed = tf.transpose(k, perm = [0, 1, 3, 2])\n",
        "  scaled = tf.matmul(q, k_transposed) / math.sqrt(d_k)\n",
        "\n",
        "  #masking\n",
        "  mask = np.full(scaled.shape, -np.inf)\n",
        "  mask = np.triu(mask, k=1)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled = scaled + mask\n",
        "\n",
        "  attention = tf.nn.softmax(scaled, axis=-1)\n",
        "  values = tf.matmul(attention, v)\n",
        "\n",
        "  return values, attention"
      ],
      "metadata": {
        "id": "mCkCRlO3wXXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 4\n",
        "batch_size = 1\n",
        "input_dim = 512\n",
        "d_model = 512\n",
        "x = tf.random.normal( (batch_size, sequence_length, input_dim) )\n",
        "\n",
        "qkv_layer = tf.keras.layers.Dense(units=3 * d_model, activation=None, use_bias=True, input_shape=(input_dim,))\n",
        "qkv = qkv_layer(x)\n",
        "num_heads = 8\n",
        "head_dim = d_model // num_heads\n",
        "qkv = tf.reshape(qkv, (batch_size, sequence_length, num_heads, 3 * head_dim))\n",
        "qkv = tf.transpose(qkv, perm=[0, 2, 1, 3])\n",
        "q, k, v = tf.split(qkv, num_or_size_splits=3, axis=3)"
      ],
      "metadata": {
        "id": "pYK37wz_xjBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values, attention = scaled_dot_product(q, k, v, mask= True)"
      ],
      "metadata": {
        "id": "x44YXjb5xG7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention[0][0]"
      ],
      "metadata": {
        "id": "RiOYwUCXxjru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values = tf.reshape(values, (batch_size, sequence_length, num_heads * head_dim))"
      ],
      "metadata": {
        "id": "GXCQdAM7y2S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values.shape"
      ],
      "metadata": {
        "id": "RXVx-0j3zYiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_layer = tf.keras.layers.Dense(units=d_model, activation=None, use_bias=True, input_shape=(d_model,))"
      ],
      "metadata": {
        "id": "lvBAH7HVzh0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = linear_layer(values)"
      ],
      "metadata": {
        "id": "e5l7Asl0zwE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "id": "Dn9lLyeVz30P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Full Code Together**"
      ],
      "metadata": {
        "id": "Jqvm2UN0R1fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import tensorflow as tf\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "  d_k = q.shape[-1]\n",
        "  k_transposed = tf.transpose(k, perm = [0, 1, 3, 2])\n",
        "  scaled = tf.matmul(q, k_transposed) / math.sqrt(d_k)\n",
        "\n",
        "  #masking\n",
        "  mask = np.full(scaled.shape, -np.inf)\n",
        "  mask = np.triu(mask, k=1)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled = scaled + mask\n",
        "\n",
        "  attention = tf.nn.softmax(scaled, axis=-1)\n",
        "  values = tf.matmul(attention, v)\n",
        "\n",
        "  return values, attention\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, input_dim, d_model, num_heads):\n",
        "    super().__init__\n",
        "    self.input_dim = input_dim\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_model // num_heads\n",
        "    self.qkv_layer = tf.keras.layers.Dense(units=3 * d_model, activation=None, use_bias=True, input_shape=(input_dim,))\n",
        "    self.linear_layer = tf.keras.layers.Dense(units=d_model, activation=None, use_bias=True, input_shape=(d_model,))\n",
        "\n",
        "  def forward(self, x, mask = None):\n",
        "    batch_size, sequence_length, input_dim = x.shape\n",
        "    print(f\"x.shape : {x.shape}\")\n",
        "    qkv = self.qkv_layer(x)\n",
        "    print(f\"qkv.shape : {qkv.shape}\")\n",
        "    qkv = tf.reshape(qkv, (batch_size, sequence_length, self.num_heads, 3 * self.head_dim))\n",
        "    print(f\"qkv.shape : {qkv.shape}\")\n",
        "    qkv = tf.transpose(qkv, perm=[0, 2, 1, 3])\n",
        "    print(f\"qkv.shape : {qkv.shape}\")\n",
        "    q, k, v = tf.split(qkv, num_or_size_splits=3, axis=3)\n",
        "    print(f\"q shape : {q.shape}, k shape : {k.shape}, v shape : {v.shape}\")\n",
        "    values, attention = scaled_dot_product(q, k, v, mask= None)\n",
        "    print(f\"values.shape : {values.shape}, attention.shape : {attention.shape}\")\n",
        "    values = tf.reshape(values, (batch_size, sequence_length, self.num_heads * self.head_dim))\n",
        "    print(f\"values.shape : {values.shape}\")\n",
        "    out = self.linear_layer(values)\n",
        "    print(f\"out.shape : {out.shape}\")\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "z2tN2uweR67I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 1024\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "\n",
        "batch_size = 30\n",
        "sequence_length = 5\n",
        "x = tf.random.normal( (batch_size, sequence_length, input_dim) )\n",
        "\n",
        "model = MultiHeadAttention(input_dim, d_model, num_heads)\n",
        "out = model.forward(x)"
      ],
      "metadata": {
        "id": "cqCJPfFaZS3a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}