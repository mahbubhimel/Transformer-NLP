{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1Ds4aWD70yGQ1-yERmN_FM_7HR22bTq2i",
      "authorship_tag": "ABX9TyM+/wmcp2J7yxF2VdDpQRcZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZlINe5zw5zX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/CoEdit/train_set.csv')\n",
        "df = df[: 100]"
      ],
      "metadata": {
        "id": "PumGp3XMxG3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import zipfile\n",
        "# import os\n",
        "\n",
        "# # Specify the path to the zip file in Google Drive\n",
        "# zip_path = '/content/drive/MyDrive/Datasets/CoEdit/vocabs.zip'\n",
        "\n",
        "# # Specify the directory to extract the contents\n",
        "# extract_dir = '/content/drive/MyDrive/Datasets/CoEdit/vocabs'\n",
        "\n",
        "# # Create a ZipFile object and extract the contents\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(extract_dir)\n",
        "\n",
        "# # List the contents of the extracted directory\n",
        "# extracted_files = os.listdir(extract_dir)\n",
        "# print(extracted_files)"
      ],
      "metadata": {
        "id": "gX9-jBGTM1Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_vocab_file_path = '/content/drive/MyDrive/Datasets/CoEdit/vocabs/source_vocab.txt'\n",
        "target_vocab_file_path = '/content/drive/MyDrive/Datasets/CoEdit/vocabs/target_vocab.txt'"
      ],
      "metadata": {
        "id": "SXqJPVR4Kav8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the contents of source_vocab.txt\n",
        "with open(source_vocab_file_path, 'r') as source_file:\n",
        "    source_vocab = [line.strip() for line in source_file.readlines()]\n",
        "\n",
        "# Read the contents of target_vocab.txt\n",
        "with open(target_vocab_file_path, 'r') as target_file:\n",
        "    target_vocab = [line.strip() for line in target_file.readlines()]"
      ],
      "metadata": {
        "id": "75er_DmFQVFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(target_vocab), len(source_vocab)"
      ],
      "metadata": {
        "id": "l2mrP_uiLPZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_source = {k:v for k,v in enumerate(source_vocab)}\n",
        "source_to_index = {v:k for k,v in enumerate(source_vocab)}\n",
        "index_to_target= {k:v for k,v in enumerate(target_vocab)}\n",
        "target_to_index = {v:k for k,v in enumerate(target_vocab)}"
      ],
      "metadata": {
        "id": "mNWOhrlc5RnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(target_to_index)"
      ],
      "metadata": {
        "id": "NQr2wQrM5o-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_sentences = list(df['source sentence'].values)\n",
        "target_sentences = list(df['target sentence'].values)"
      ],
      "metadata": {
        "id": "HT18dmMc7W0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(source_sentences)"
      ],
      "metadata": {
        "id": "YGij55EC75Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(len(x) for x in source_sentences), max(len(x) for x in target_sentences),"
      ],
      "metadata": {
        "id": "b5sdDYIs8aYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len('Fr xmpl, cntrs wth  lt f dsrts cn trnsfrm thr dsrt t ncrs thr hbtbl lnd nd s rrgtn t prvd cln wtr t th dsrt.')"
      ],
      "metadata": {
        "id": "su9CXzjf-out"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PERCENTILE = 97\n",
        "print( f\"{PERCENTILE}th percentile length in Source: {np.percentile([len(x) for x in source_sentences], PERCENTILE)}\" )\n",
        "print( f\"{PERCENTILE}th percentile length in Target: {np.percentile([len(x) for x in target_sentences], PERCENTILE)}\" )"
      ],
      "metadata": {
        "id": "n2uUHHhc8vOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(source_vocab[42])"
      ],
      "metadata": {
        "id": "N_tUq1is5NYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_vocab[7438]"
      ],
      "metadata": {
        "id": "ZNHOt9MQnXmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_x = ['a']"
      ],
      "metadata": {
        "id": "PJ5MvCgp3r8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tk = \"'\"\n",
        "if tk in source_vocab:\n",
        "  print(\"yes\")\n",
        "else:\n",
        "  print(\"No\")"
      ],
      "metadata": {
        "id": "NoAc8JeU2fxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_length = 200\n",
        "\n",
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        # print(f\"token: {token}\")\n",
        "        if token not in vocab and token != ' ':\n",
        "            print(f\"not found token: {token}\")\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
        "\n",
        "valid_sentence_indicies = []\n",
        "for index in range(len(source_sentences)):\n",
        "    # print(index)\n",
        "    source_sentence, target_sentence = source_sentences[index].lower(), target_sentences[index].lower()\n",
        "    if is_valid_length(source_sentence, max_sequence_length) \\\n",
        "      and is_valid_length(target_sentence, max_sequence_length) \\\n",
        "      and is_valid_tokens(source_sentence, source_vocab):\n",
        "        valid_sentence_indicies.append(index)\n",
        "\n",
        "print(f\"Number of sentences: {len(source_sentences)}\")\n",
        "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")"
      ],
      "metadata": {
        "id": "SqOlHzZ_sHYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_sentences = [source_sentences[i] for i in valid_sentence_indicies]\n",
        "target_sentences = [target_sentences[i] for i in valid_sentence_indicies]"
      ],
      "metadata": {
        "id": "QcvM2LDtt2Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(source_sentences), len(target_sentences)"
      ],
      "metadata": {
        "id": "3wKW3grvsSzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_sentences[:3]"
      ],
      "metadata": {
        "id": "EjHJUnDos0ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_sentences[: 3]"
      ],
      "metadata": {
        "id": "UcoBM8sosZtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, source_sentences, target_sentences):\n",
        "        self.source_sentences = source_sentences\n",
        "        self.target_sentences = target_sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.source_sentences[idx], self.target_sentences[idx]"
      ],
      "metadata": {
        "id": "TvmEJyHAt-zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TextDataset(source_sentences, target_sentences)"
      ],
      "metadata": {
        "id": "dnMvKcU7uN8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.__len__()"
      ],
      "metadata": {
        "id": "jg8HeomOucNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.__getitem__(0)"
      ],
      "metadata": {
        "id": "6q6Lxf85uuSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 3\n",
        "train_loader = DataLoader(dataset, batch_size)\n",
        "iterator = iter(train_loader)"
      ],
      "metadata": {
        "id": "nJm-qjlzvCPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_num, batch in enumerate(iterator):\n",
        "    print(batch)\n",
        "    print(batch_num)\n",
        "    if batch_num > 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "jgfb1gP8vnIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PADDING_TOKEN = '[PAD]'\n",
        "UNKNOWN_TOKEN = '[UNK]'\n",
        "START_TOKEN = '[START]'\n",
        "END_TOKEN = '[END]'"
      ],
      "metadata": {
        "id": "wLVzOauqw59v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_to_index['##igan']\n",
        "# index_to_target[7438]"
      ],
      "metadata": {
        "id": "q5nXlS_33Tb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch[1]"
      ],
      "metadata": {
        "id": "UaeNYGpZ4I85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.transforms import BERTTokenizer\n",
        "source_tokenizer = BERTTokenizer(vocab_path=source_vocab_file_path, do_lower_case=True, return_tokens=True)\n",
        "target_tokenizer = BERTTokenizer(vocab_path=target_vocab_file_path, do_lower_case=True, return_tokens=True)"
      ],
      "metadata": {
        "id": "J57-9J9T3tCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentence, language_to_index, tokenizer_type, start_token=True, end_token=True):\n",
        "\n",
        "    if tokenizer_type == 'src_tkn':\n",
        "        src_sentence_tokens = source_tokenizer(sentence)\n",
        "        sentence_word_indicies = [language_to_index[token] for token in src_sentence_tokens]\n",
        "    if tokenizer_type == 'tgt_tkn':\n",
        "        tgt_sentence_tokens = target_tokenizer(sentence)\n",
        "        sentence_word_indicies = [language_to_index[token] for token in tgt_sentence_tokens]\n",
        "\n",
        "    if start_token:\n",
        "        sentence_word_indicies.insert(0, language_to_index[START_TOKEN])\n",
        "    if end_token:\n",
        "        sentence_word_indicies.append(language_to_index[END_TOKEN])\n",
        "\n",
        "    for _ in range(len(sentence_word_indicies), max_sequence_length):\n",
        "        sentence_word_indicies.append(language_to_index[PADDING_TOKEN])\n",
        "\n",
        "    return torch.tensor(sentence_word_indicies)"
      ],
      "metadata": {
        "id": "c4-KLRh9xLm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_tokenized, target_tokenized = [], []\n",
        "for sentence_num in range(batch_size):\n",
        "    source_sentence, target_sentence = batch[0][sentence_num], batch[1][sentence_num]\n",
        "    source_tokenized.append( tokenize(source_sentence, source_to_index, tokenizer_type = 'src_tkn', start_token=False, end_token=False) )\n",
        "    target_tokenized.append( tokenize(target_sentence, target_to_index, tokenizer_type = 'tgt_tkn', start_token=True, end_token=True) )\n",
        "source_tokenized = torch.stack(source_tokenized)\n",
        "target_tokenized = torch.stack(target_tokenized)"
      ],
      "metadata": {
        "id": "IEjIUNoc4Xft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(target_tokenized), len(source_tokenized)"
      ],
      "metadata": {
        "id": "jAauwe9DIPku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_tokenized"
      ],
      "metadata": {
        "id": "hlZHtQPgTPwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(batch[0])"
      ],
      "metadata": {
        "id": "KLL4Wu1JbG6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NEG_INFTY = -1e9\n",
        "\n",
        "def create_masks(source_batch, target_batch):\n",
        "    num_sentences = len(source_batch)\n",
        "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n",
        "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
        "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "      source_sentence_length, target_sentence_length = len(source_batch[idx]), len(target_batch[idx])\n",
        "      source_wordPiece_to_padding_mask = np.arange(source_sentence_length + 1, max_sequence_length)\n",
        "      target_wordPiece_to_padding_mask = np.arange(target_sentence_length + 1, max_sequence_length)\n",
        "      encoder_padding_mask[idx, :, source_wordPiece_to_padding_mask] = True\n",
        "      encoder_padding_mask[idx, source_wordPiece_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_self_attention[idx, :, target_wordPiece_to_padding_mask] = True\n",
        "      decoder_padding_mask_self_attention[idx, target_wordPiece_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_cross_attention[idx, :, source_wordPiece_to_padding_mask] = True\n",
        "      decoder_padding_mask_cross_attention[idx, target_wordPiece_to_padding_mask, :] = True\n",
        "\n",
        "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
        "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
        "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
        "    print(f\"encoder_self_attention_mask {encoder_self_attention_mask.size()}: {encoder_self_attention_mask[0, :10, :10]}\")\n",
        "    print(f\"decoder_self_attention_mask {decoder_self_attention_mask.size()}: {decoder_self_attention_mask[0, :10, :10]}\")\n",
        "    print(f\"decoder_cross_attention_mask {decoder_cross_attention_mask.size()}: {decoder_cross_attention_mask[0, :10, :10]}\")\n",
        "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
      ],
      "metadata": {
        "id": "RYocpp0zZGQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_masks(batch[0], batch[1])"
      ],
      "metadata": {
        "id": "bEZ1wSiZdkbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class SentenceEmbedding(nn.Module):\n",
        "#     \"For a given sentence, create an embedding\"\n",
        "#     def __init__(self, max_sequence_length, d_model, language_to_index, source_tokenizer, target_tokenizer,START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "#         super().__init__()\n",
        "#         self.vocab_size = len(language_to_index)\n",
        "#         self.max_sequence_length = max_sequence_length\n",
        "#         self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
        "#         self.language_to_index = language_to_index\n",
        "#         self.source_tokenizer = source_tokenizer\n",
        "#         self.target_tokenizer = target_tokenizer\n",
        "#         self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
        "#         self.dropout = nn.Dropout(p=0.1)\n",
        "#         self.START_TOKEN = START_TOKEN\n",
        "#         self.END_TOKEN = END_TOKEN\n",
        "#         self.PADDING_TOKEN = PADDING_TOKEN\n",
        "\n",
        "#     def batch_tokenize(self, batch, start_token=True, end_token=True):\n",
        "\n",
        "#         def tokenize(sentence, tokenizer_type, start_token=True, end_token=True):\n",
        "\n",
        "#           if tokenizer_type == 'src_tkn':\n",
        "#               src_sentence_tokens = self.source_tokenizer(sentence)\n",
        "#               sentence_word_indicies = [self.language_to_index[token] for token in src_sentence_tokens]\n",
        "#           if tokenizer_type == 'tgt_tkn':\n",
        "#               tgt_sentence_tokens = self.target_tokenizer(sentence)\n",
        "#               sentence_word_indicies = [self.language_to_index[token] for token in tgt_sentence_tokens]\n",
        "\n",
        "#           if start_token:\n",
        "#               sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])\n",
        "#           if end_token:\n",
        "#               sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n",
        "\n",
        "#           for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n",
        "#               sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])\n",
        "\n",
        "#           return torch.tensor(sentence_word_indicies)\n",
        "\n",
        "#         tokenized = []\n",
        "#         for sentence_num in range(len(batch)):\n",
        "#            tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n",
        "#         tokenized = torch.stack(tokenized)\n",
        "#         return tokenized.to(get_device())\n",
        "\n",
        "#     def forward(self, x, end_token=True): # sentence\n",
        "#         x = self.batch_tokenize(x ,end_token)\n",
        "#         x = self.embedding(x)\n",
        "#         pos = self.position_encoder().to(get_device())\n",
        "#         x = self.dropout(x + pos)\n",
        "#         return x"
      ],
      "metadata": {
        "id": "2z1FJsm1-QTb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}